{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Global variable for Multi-Cancer dataset's filepath\n",
    "base_dir = 'Multi-Cancer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_subclass_directories(root_dir, exclude_dir):\n",
    "    \"\"\"\n",
    "    Generates a list of (class, subclass) directory tuples from the root directory,\n",
    "    excluding the specified directory.\n",
    "    \"\"\"\n",
    "    class_subclass_dirs = []\n",
    "    for class_dir in os.listdir(root_dir):\n",
    "        class_path = os.path.join(root_dir, class_dir)\n",
    "        if os.path.isdir(class_path) and class_dir.lower() != exclude_dir.lower():\n",
    "            for subclass_dir in os.listdir(class_path):\n",
    "                subclass_path = os.path.join(class_path, subclass_dir)\n",
    "                if os.path.isdir(subclass_path):\n",
    "                    class_subclass_dirs.append((class_dir, subclass_dir))\n",
    "    return class_subclass_dirs\n",
    "\n",
    "filtered_dirs = get_class_subclass_directories(base_dir, 'all')\n",
    "\n",
    "# Print out the directories being used\n",
    "for class_dir, subclass_dir in filtered_dirs:\n",
    "    print(f\"Class: {class_dir}, Subclass: {subclass_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_model(num_classes):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(200, 200, 3)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Dropout(0.2),  # Adjusted dropout rate\n",
    "\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Dropout(0.2),  # Adjusted dropout rate\n",
    "\n",
    "        Flatten(),\n",
    "\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.4),  # Adjusted dropout rate\n",
    "\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    adam_optimizer = Adam(learning_rate=0.001)  \n",
    "\n",
    "    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "def split_data(base_dir, class_dir, subclass, train_size=0.75, val_size=0.15):\n",
    "    \"\"\"\n",
    "    Split data into training, validation, and testing sets for a given class and subclass.\n",
    "    \"\"\"\n",
    "    # Paths for source and split directories\n",
    "    source_dir = os.path.join(base_dir, class_dir, subclass)\n",
    "    train_dir = os.path.join(base_dir, class_dir, 'train', subclass)\n",
    "    val_dir = os.path.join(base_dir, class_dir, 'val', subclass)\n",
    "    test_dir = os.path.join(base_dir, class_dir, 'test', subclass)\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # List all files in the source directory\n",
    "    files = os.listdir(source_dir)\n",
    "    np.random.shuffle(files)\n",
    "\n",
    "    # Split files\n",
    "    total_files = len(files)\n",
    "    train_end = int(total_files * train_size)\n",
    "    val_end = train_end + int(total_files * val_size)\n",
    "\n",
    "    # Copy files to respective directories\n",
    "    for i, file in enumerate(files):\n",
    "        if i < train_end:\n",
    "            shutil.copy(os.path.join(source_dir, file), train_dir)\n",
    "        elif i < val_end:\n",
    "            shutil.copy(os.path.join(source_dir, file), val_dir)\n",
    "        else:\n",
    "            shutil.copy(os.path.join(source_dir, file), test_dir)\n",
    "\n",
    "# Loop through each class-subclass pair\n",
    "for class_dir, subclass in filtered_dirs:\n",
    "    print(f\"Class: {class_dir}, Subclass: {subclass}\")\n",
    "    split_data(base_dir, class_dir, subclass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "def create_generators(base_dir, class_dir, batch_size, target_size=(200, 200)):\n",
    "    print(\"Batch size received in create_generators:\", batch_size)\n",
    "    # Rest of the function code\n",
    "    \"\"\"\n",
    "    Create training, validation, and testing generators.\n",
    "    \"\"\"\n",
    "    # Data generator with augmentation for training\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.15,\n",
    "        height_shift_range=0.15,\n",
    "        shear_range=0.15,\n",
    "        zoom_range=0.15,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    # Data generator without augmentation for validation and testing\n",
    "    val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Training generator\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        os.path.join(base_dir, class_dir, 'train'),\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Validation generator\n",
    "    validation_generator = val_test_datagen.flow_from_directory(\n",
    "        os.path.join(base_dir, class_dir, 'val'),\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "\n",
    "    # Testing generator\n",
    "    test_generator = val_test_datagen.flow_from_directory(\n",
    "        os.path.join(base_dir, class_dir, 'test'),\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False  # Usually, we don't shuffle the test data\n",
    "    )\n",
    "\n",
    "    return train_generator, validation_generator, test_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=1, mode='min')\n",
    "trained = []\n",
    "def train_model_for_each_class(base_dir, class_subclass_dirs):\n",
    "    for class_dir, _ in set([(class_dir, _) for class_dir, _ in class_subclass_dirs]):\n",
    "        \n",
    "        if(class_dir in trained):\n",
    "            continue\n",
    "        trained.append(class_dir)\n",
    "        print(f\"Training model for {class_dir}\")\n",
    "\n",
    "        train_subdir = os.path.join(base_dir, class_dir, 'train')\n",
    "\n",
    "        if os.path.exists(train_subdir):\n",
    "            num_classes = len(os.listdir(train_subdir))\n",
    "        else:\n",
    "            raise ValueError(f\"Train directory for '{class_dir}' not found.\")\n",
    "\n",
    "        # Create a new instance of the model for this class\n",
    "        model = create_model(num_classes)\n",
    "        \n",
    "        # Create data generators for the current class\n",
    "        train_generator, validation_generator, test_generator = create_generators(base_dir, class_dir, 16) \n",
    "        print(\"Training Generator - batch size:\", train_generator.batch_size, \"samples:\", train_generator.n)\n",
    "        print(\"Validation Generator - batch size:\", validation_generator.batch_size, \"samples:\", validation_generator.n)\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            validation_data=validation_generator,\n",
    "            epochs=50,\n",
    "            callbacks=[early_stopping, reduce_lr]\n",
    "        )\n",
    "\n",
    "        # Save the model\n",
    "        model_path = f'model_{class_dir}.h5'\n",
    "        model.save(model_path)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "        print(f\"Test Loss for {class_dir}: {test_loss}\")\n",
    "        print(f\"Test Accuracy for {class_dir}: {test_accuracy}\")\n",
    "\n",
    "train_model_for_each_class(base_dir, filtered_dirs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
